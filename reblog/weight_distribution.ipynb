{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Weights in a Network\n",
    "\n",
    "Let us consider the simplest possible neural network, 1 input $x$, 1 output $y$ with some non-linearity $f$. This is expressed as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y = f(wx + b)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $w$, $b$ are the weight and bias in the network. Putting this into a slightly different form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y = f(w(x + b/w))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "we know that the activation function is centered at $-b/w$.\n",
    "\n",
    "For this experiment, we look at the distribution of $-b/w$ for a swarm fitting to a\n",
    "\n",
    "- trig function: sin and cos have very obvious turning points.\n",
    "- ReLU activation: as a very simple activation, the $-b/w$ will correspond exactly to the turning points\n",
    "- Single hidden layer: this makes interpretability a bit clearer since we have a clearer understanding of the mix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from swarm import core, animator, networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bee_trainer(xt, yt, width=2, num_epochs=200):\n",
    "    \"\"\"Define a simple training loop for use with swarm\"\"\"\n",
    "    net = networks.flat_net(1, width, activation=nn.ReLU)\n",
    "\n",
    "    optimiser = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimiser.zero_grad()\n",
    "        ypred = net(xt)\n",
    "\n",
    "        loss = loss_func(ypred, yt)\n",
    "        if torch.isnan(loss):\n",
    "            raise RuntimeError(\"NaN loss, poorly configured experiment\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        weight, bias, *_ = net.parameters()\n",
    "        yield ypred, weight.detach().flatten().numpy().copy(), bias.detach().numpy().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4\n",
    "NUM_BEES = 5\n",
    "SEED = 20\n",
    "\n",
    "def main():\n",
    "\n",
    "    xt = torch.linspace(-3 * np.pi, 3 * np.pi, 101)\n",
    "    yt = torch.sin(xt)\n",
    "\n",
    "    bp = {\"xt\": xt, \"yt\": yt, \"width\": 20, \"num_epochs\": NUM_EPOCHS}\n",
    "    res = core.swarm_train(bee_trainer, bp, num_bees=NUM_BEES, fields=\"ypred,weights,biases\", seed=SEED)\n",
    "\n",
    "    bw = -res[\"biases\"] / res[\"weights\"]\n",
    "\n",
    "    # reduce range to be safe\n",
    "    bw = bw.clip(-10, 10)\n",
    "\n",
    "\n",
    "    ls = animator.LineSwarm.standard(xt.detach().numpy(), yt.detach().numpy(), res[\"ypred\"][::10], set_xlim=(-10,10))\n",
    "    hist = animator.HistogramSwarm.from_swarm(\n",
    "        bw, 100, set_title=\"- Biases/Weights\", set_ylabel=\"Count\", set_xlim=(-10,10)\n",
    "    )\n",
    "    animator.swarm_animate([ls, hist], \"_build/html/_images/weight_distr.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to weight_distr.mp4\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls>\n",
       "  <source src=\"weight_distr.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video controls>\n",
    "  <source src=\"_images/weight_distr.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Distributions\n",
    "\n",
    "We can see that the biases and weights cluster around the places where the sin curve turns. As you'd expect with the starting conditions being quite close to 0, we see that most of the bends assigned by the network fit into the first curves and not the turning points at extremities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
